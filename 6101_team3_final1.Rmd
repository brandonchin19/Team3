---
title: "DATS 6101 Final Project"
author: "Brandon Chin, Paul Kelly, Ksenia Shadrina, Luke Wu"
date: "4/11/22"
# date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
library(tidyverse)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
#options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

```{r modify_outlierKD2}
#' Original outlierKD function by By Klodian Dhana,
#' https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
#' Modified to have third argument for removing outliers instead of interactive prompt,
#' and after removing outlier, original df will not be changed. The function returns the a df,
#' which can be saved as original df name if desired.
#' Also added QQ-plot in the output, with options to show/hide boxplot, histogram, qqplot.
#' Check outliers, and option to remove them, save as a new dataframe.
#' @param df The dataframe.
#' @param var The variable in the dataframe to be checked for outliers
#' @param rm Boolean. Whether to remove outliers or not.
#' @param boxplt Boolean. Whether to show the boxplot, before and after outliers removed.
#' @param histogram Boolean. Whether to show the histogram, before and after outliers removed.
#' @param qqplt Boolean. Whether to show the qqplot, before and after outliers removed.
#' @return The dataframe with outliers replaced by NA if rm==TRUE, or df if nothing changed
#' @examples
#'   outlierKD2(mydf, height, FALSE, TRUE, TRUE, TRUE)
#'   mydf = outlierKD2(mydf, height, TRUE, TRUE, TRUE, TRUE)
#'   mydfnew = outlierKD2(mydf, height, TRUE)
#' @export
outlierKD2 <- function(df, var, rm=TRUE, boxplt=TRUE, histogram=TRUE, qqplt=TRUE) {
  dt = df # duplicate the dataframe for potential alteration
  var_name <- eval(substitute(var),eval(dt))
  na1 <- sum(is.na(var_name))
  m1 <- mean(var_name, na.rm = T)
  colTotal <- boxplt+histogram+qqplt
  par(mfrow=c(2, max(2,colTotal)), oma=c(0,0,3,0)) # fixed issue with only 0 or 1 chart selected
  if (qqplt) {
    qqnorm(var_name, main = "With outliers")
    qqline(var_name)
  }
  if (histogram) { hist(var_name, main="With outliers", xlab=NA, ylab=NA) }
  if (boxplt) { boxplot(var_name, main="With outliers") }

  outlier <- boxplot.stats(var_name)$out
  mo <- mean(outlier)
  var_name <- ifelse(var_name %in% outlier, NA, var_name)
  if (qqplt) {
    qqnorm(var_name, main = "Without outliers")
    qqline(var_name)
  }
  if (histogram) { hist(var_name, main="Without outliers", xlab=NA, ylab=NA) }
  if (boxplt) { boxplot(var_name, main="Without outliers") }
  
  if(colTotal > 0) {  # if no charts are wanted, skip this section
    title("Outlier Check", outer=TRUE)
    na2 <- sum(is.na(var_name))
    cat("Outliers identified:", na2 - na1, "\n")
    cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "\n")
    cat("Mean of the outliers:", round(mo, 2), "\n")
    m2 <- mean(var_name, na.rm = T)
    cat("Mean without removing outliers:", round(m1, 2), "\n")
    cat("Mean if we remove outliers:", round(m2, 2), "\n")
  }

  # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
  # if(response == "y" | response == "yes"){
  if(rm){
      dt[as.character(substitute(var))] <- invisible(var_name)
      #assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
      cat("Outliers successfully removed", "\n")
      return(invisible(dt))
  } else {
      cat("Nothing changed", "\n")
      return(invisible(df))
  }
}

```

### Research Topic:

There is considerable evidence indicating lending disparities throughout the United States. (Steil et. al: 2018). For our research topic, we will explore lending practices in one of the fastest appreciating real estate markets over the past thirty years - the state of California. Specifically, we aim to look at the factors that are associated with denials for non-commercial mortgage loans.

Our SMART question is:

“Which factors drove denials for mortgages in California in 2019?”

To answer this question, we are using the Federal Financial Institutions Examination Council's (FFIEC) Home Mortgage Disclosure Act (HMDA) dataset from 2019, located here: https://ffiec.cfpb.gov/data-publication/dynamic-national-loan-level-dataset/2019. We are  focusing on a subset of data of 10,000 observations from 2019 that we will further filter on California leaving us with 5,196 observations.

Our Github repository address is:https://github.com/brandonchin19/Team3/.

### Load in dataset

```{r, include=FALSE}
hmda_ca <- data.frame(read.csv("hmda_ca_new.csv"))
str(hmda_ca)
```

### Check number of rows

```{r}
dim(hmda_ca)
```


### Check head and tail of dataframe

```{r}
xkabledplyhead(hmda_ca,5)
xkabledplytail(hmda_ca,5)
```


### Subsetting to non-business properties; principal residences only

```{r}
hmda_ca <- subset(hmda_ca,business_or_commercial_purpose=="2")
str(hmda_ca)
```


```{r}
hmda_ca <- subset(hmda_ca,business_or_commercial_purpose=="2")
str(hmda_ca) #48030 obs. of  99 variables
```

### Subsetting principal residences only; tail and head check to make sure that the geography is widespread/our sample is "random"; there are now 59 distinct counties.

```{r}
hmda_ca <- subset(hmda_ca,occupancy_type=="1")
dim(hmda_ca) #45735    99
loadPkg("sqldf")
sqldf("select count(distinct(county_code)) from hmda_ca")
unloadPkg("sqldf")
```

### Subsetting to only relevant actions: denial or approval

```{r}
hmda_ca1<-hmda_ca%>%filter(action_taken %in% c("1", "3"))
hmda_ca<-hmda_ca1
dim(hmda_ca) #30661    99
```


### Subsetting relevant variables for answering the SMART question
```{r}
hmda_ca_final <- hmda_ca[c(10,11,12,13,22,24,39,46,74,78)]
str(hmda_ca_final) #30661 obs. of  10 variables:
```
##Clean the age and debt-to-income ratio group variable (some "8888" value)
```{r}
unique(hmda_ca_final$applicant_age)
hmda_age<-hmda_ca_final%>%filter(applicant_age !="8888")
unique(hmda_age$applicant_age)
hmda_ca_final<-hmda_age

```

### Changing vector types

```{r}
hmda_ca_final_1 = hmda_ca_final
hmda_ca_final_1$derived_ethnicity = factor(hmda_ca_final$derived_ethnicity)
hmda_ca_final_1$derived_race = factor(hmda_ca_final$derived_race)
hmda_ca_final_1$derived_sex = factor(hmda_ca_final$derived_sex)
#hmda_ca_final_1$action_taken = factor(hmda_ca_final$action_taken)
hmda_ca_final_1$loan_amount = as.numeric(hmda_ca_final$loan_amount)
hmda_ca_final_1$interest_rate = as.numeric(hmda_ca_final$interest_rate)
hmda_ca_final_1$property_value = as.numeric(hmda_ca_final$property_value)
hmda_ca_final_1$income = as.numeric(hmda_ca_final$income)
hmda_ca_final_1$applicant_age = factor(hmda_ca_final$applicant_age)
#hmda_ca_final_1$applicant_ethnicity.1=factor(hmda_ca_final$applicant_age)
str(hmda_ca_final_1)
```

#Examine and filter out the missing values 

```{r, include=TRUE}
missvalue <- is.na(hmda_ca_final_1)
summary(missvalue)
#There are 30659 observations after we've filtered on all of the relevant fields. 
#Missing values are present in interest_rate: 6917; property_value: 1733, and income: 1302
```


```{r, include=TRUE}
#a <- na.omit(hmda_ca_final_1, cols="income")
#missvalue1 <- is.na(a)
#summary(missvalue1)
hmda_ca_final_1$interest_rate[is.na(hmda_ca_final_1$interest_rate)] <- mean(hmda_ca_final_1$interest_rate, na.rm = TRUE)
missvalue1 <- is.na(hmda_ca_final_1)
#summary(missvalue1)

hmda_ca_final_1$income[is.na(hmda_ca_final_1$income)] <- mean(hmda_ca_final_1$income, na.rm = TRUE)
missvalue2 <- is.na(hmda_ca_final_1)
summary(missvalue2)

hmda_ca_final_2 <- na.omit(hmda_ca_final_1, cols="property_value")
missvalue3 <- is.na(hmda_ca_final_2)
summary(missvalue3)
#After cleaning out the missing values, we are left with 28926  observations
```
##Run initial summary statistics

```{r, results='show'}
options(scipen=9, digits = 3) 
Numerical_var <- subset(hmda_ca_final_2,select=c(loan_amount, income, property_value, interest_rate))
library(kableExtra)
summary_t<-kbl(summary(Numerical_var))%>%
  kable_styling()
summary_t

#No NAs
```



### VI. Exploratory Data Analysis

#Exploring the categorical values first

```{r, include=TRUE}
library(ggplot2)
ggplot(hmda_ca_final_2, aes(x = factor(derived_sex), fill=derived_sex)) +
    geom_bar()+
  labs(title="Graph 1. Applicant sex distribution", x="Applicant Sex", y="Count")

ggplot(hmda_ca_final_2, aes(x = factor(applicant_age),fill=applicant_age)) +
    geom_bar()+
  labs(title="Graph 2. Applicant age distribution", x="Applicant Age", y="Count")

ggplot(hmda_ca_final_2, aes(x = factor(derived_ethnicity), fill=derived_ethnicity)) +geom_bar()+
  labs(title="Graph 3. Applicant ethnicity distribution", x="Applicant Ethnicity", y="Count")

ggplot(hmda_ca_final_2, aes(x = factor(action_taken), fill=action_taken)) +
    geom_bar()+
  labs(title="Graph 4. Action taken distribution", x="Action Taken", y="Count")

ggplot(hmda_ca_final_2, aes(x = factor(derived_race), fill=derived_race)) +
    geom_bar()+
  labs(title="Graph 5. Derived Race", x="Race of the Applicant", y= "Count")+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))


```


Next, we proceed to exploring the numerical data for normality and outliers. Graphs 8-10 indicate that none of the numerical variables are normally distributed.

```{r, include=TRUE}
ggplot(hmda_ca_final_2,aes(x=loan_amount))+
  geom_histogram(color="black", fill="steelblue")+
  labs(title=" Graph 6. Histogram of Loan Amount", x="Loan Amount in dollars", y="Frequency")

ggplot(hmda_ca_final_2,aes(x=interest_rate))+
  geom_histogram(color="black", fill="pink")+
  labs(title="Graph 8. Histogram of Interest Rate", x="Interest Rates", y="Frequency")

ggplot(hmda_ca_final_2,aes(x=income))+
  geom_histogram(color="black", fill="azure3")+
  labs(title="Graph 9. Histogram of Income", x="Income in dollars", y="Frequency")

ggplot(hmda_ca_final_2,aes(x=property_value))+
  geom_histogram(color="black", fill="lightblue")+
  labs(title="Graph 10. Histogram of Property Values", x="Property Values in dollars", y="Frequency")


```


As another check, we inspect qqnorm plots and confirm that the distributions are not normal(Graph 11-14)

```{r, include=TRUE}
qqnorm(hmda_ca_final_2$interest_rate,
       main="Graph 11. QQ Plot of Interest Rates",
       ylab="Interest Rate",
       col="pink")
qqline(hmda_ca_final_2$interest_rate)

qqnorm(hmda_ca_final_2$income,
       main="Graph 13. QQ Plot of Income",
       ylab="Income",
       col="green")
qqline(hmda_ca_final_2$income)

qqnorm(hmda_ca_final_2$property_value,
       main="Graph 12. QQ Plot of Property Values",
       ylab="Property Value",
       col="blue")
qqline(hmda_ca_final_2$property_value)

qqnorm(hmda_ca_final_2$loan_amount,
       main="Graph 14.QQ Plot of Loan Amount",
       ylab="Loan Amount",
       col="purple")
qqline(hmda_ca_final_1$loan_amount)
```


Next, we remove outliers and check for normality again.
```{r, include=TRUE}
hmda_ca_final_no_outliers_3 <- outlierKD2(hmda_ca_final_2,interest_rate)


missvalue4 <- is.na(hmda_ca_final_no_outliers_3)
summary(missvalue4)
```

```{r, include=TRUE}
#hmda_ca_final_no_outliers_4 <- outlierKD2(hmda_ca_final_no_outliers_3,property_value)
boxplot(hmda_ca_final_no_outliers_3$property_value)
boxplot(hmda_ca_final_no_outliers_3$property_value)$out
outliers <- boxplot(hmda_ca_final_no_outliers_3$property_value, plot=FALSE)$out
hmda_ca_final_no_outliers_3[which(hmda_ca_final_no_outliers_3$property_value %in% outliers),]
hmda_ca_final_no_outliers_4<- hmda_ca_final_no_outliers_3[-which(hmda_ca_final_no_outliers_3$property_value %in% outliers),]
boxplot(hmda_ca_final_no_outliers_4$property_value)
boxplot(hmda_ca_final_no_outliers_4$property_value)$out
missvalue5 <- is.na(hmda_ca_final_no_outliers_4)
summary(missvalue5) # no missing values here
```


```{r, include=TRUE}
hmda_ca_final_no_outliers_5 <- outlierKD2(hmda_ca_final_no_outliers_4,income)
```

```{r, include=TRUE}
loans <- outlierKD2(hmda_ca_final_no_outliers_5,loan_amount)
dim(loans) # 27,168 after removing the outliers
```

#Reexamine the variables for normality after removing the outliers.

```{r, include=TRUE}
qqnorm(loans$interest_rate,
       main="Graph 15. QQ Plot of Interest Rates",
       ylab="Interest Rate",
       col="pink")
qqline(loans$interest_rate)

qqnorm(loans$property_value,
       main="Graph 16. QQ Plot of Property Values",
       ylab="Property Value",
       col="blue")
qqline(loans$property_value)

qqnorm(loans$income,
       main="Graph 17.QQ Plot of Income",
       ylab="Income",
       col="green")
qqline(loans$income)

qqnorm(loans$loan_amount,
       main="Graph 18. QQ Plot of Loan Amount",
       ylab="Loan Amount",
       col="purple")
qqline(loans$loan_amount)
```

The values do not appear to be normally distributed even after removing the outliers

```{r, include=TRUE}
Numerical_var <- subset(loans,select=c(loan_amount, income, property_value, interest_rate))
#str(Numerical_var)
library(kableExtra)
summary_t<-kbl(summary(Numerical_var))%>%
  kable_styling()
summary_t


```

```{r, remove missing values again, include=TRUE}
loans_m<-loans%>%filter(!(is.na(loan_amount)))
dim(loans_m) #27118    12
loans_m1<-loans_m%>%filter(!(is.na(income)))
dim(loans_m1) #26295    12
loans<-loans_m1%>%filter(!(is.na(interest_rate)))
dim(loans)#final #24018    12
str(loans)
View(loans)
```
However, the means and the medians are fairly close in all instances; do we keep removing the outliers?


## Starting the tests
```{r, include=TRUE}
# library(epiDisplay)
# tab1(loans$derived_race, sort.group = "decreasing", cum.percent = TRUE)
```


##Preparing data for corrplot
```{r, include=FALSE}
# summary(loans$derived_race)
# loans$Black<- ifelse(loans$derived_race=="Black or African American", 1, 0)
# loans$AIAN<- ifelse(loans$derived_race=="American Indian or Alaska Native", 1, 0)
# loans$NHPI<- ifelse(loans$derived_race=="Native Hawaiian or Other Pacific Islander", 1, 0)
# loans$Asian<- ifelse(loans$derived_race=="Asian", 1, 0)
# loans$Joint<- ifelse(loans$derived_race=="Joint", 1, 0)
# loans$N_A<- ifelse(loans$derived_race=="Race Not Available", 1, 0)
# loans$two_or<- ifelse(loans$derived_race=="2 or more minority races", 1, 0)
# str(loans)
```

```{r, include=INCLUDE}
# loans_all_num<-loans[c(4:17)]
# loans_all_num$applicant_age =as.numeric(loans$applicant_age)
# loans_all_num$applicant_ethnicity.1=as.numeric(loans$applicant_ethnicity.1)
# str(loans_all_num)
```


```{r, include=TRUE}
# cor_loans<-cor(loans_all_num, use="complete.obs")
# xkabledply(cor_loans)
# loadPkg("corrplot")
# corrplot(cor_loans)

```

### Chi-Square Test for Loan Approval and Race: result=>reject the null of independence: all tests REJECT THE NULL HYPOTHESIS OF INDEPENDENCE NOW THAT WE HAVE MORE DATA 
``` {r, results=TRUE}
contable1 = table(loans$derived_race, loans$action_taken)
xkabledply(contable1, title="Contingency table for Loan Approval and Race")
chitest1 = chisq.test(contable1)
chitest1
```

``` {r, results=TRUE}
contable2 = table(loans$derived_sex, loans$action_taken)
xkabledply(contable2, title="Contingency table for Loan Approval and Sex")
chitest2 = chisq.test(contable2)
chitest2
```

``` {r, results=TRUE}
contable3 = table(loans$derived_ethnicity, loans$action_taken)
xkabledply(contable3, title="Contingency table for Loan Approval and Ethnicity")
chitest3 = chisq.test(contable3)
chitest3
```

``` {r, results=TRUE}
contable4 = table(loans$applicant_age, loans$action_taken)
xkabledply(contable4, title="Contingency table for Loan Approval and Age")
chitest4 = chisq.test(contable4)
chitest4
```



#Adding New Code for final and downsampling

```{r}
hmda <- data.frame(loans)
str(hmda$action_taken)
hmda$approval<- ifelse(hmda$action_taken=="1", "Approved","Denied")
hmda$approval<- factor(hmda$approval)
str(hmda)
table(hmda$approval)

# Approved   Denied 
#    18542     5476 

```


Checking frequency of approval variable and derived_race variable

```{r}
with(hmda,
     {
       print(table(derived_race))
       print(table(approval))
     }
)




```
Creating DownSample of hmda dataset, 
the Approved far outnumber the Denied making this dataset severely unbalanced 
```{r}
Approved<- which(hmda$action_taken=="1")
Denied<- which(hmda$action_taken=="3")

length(Approved)
length(Denied)

#The technique
approved_downsample<-sample(Approved,length(Denied))
hmda_down<- hmda[c(approved_downsample, Denied),]
str(hmda_down)

```

#3Balanced data are down to 10952 obs
```{r}

hmda_downnum<-hmda_down[c(4:11, 13:17, 19)]
hmda_downnum$approved<- ifelse(hmda_downnum$action_taken=="1", 1, 0)
hmda_downnum$denied<- ifelse(hmda_downnum$action_taken=="3", 1, 0)

str(hmda_downnum)
```


```{r}
cor_loans<-cor(hmda_downnum, use="complete.obs")
xkabledply(cor_loans)
loadPkg("corrplot")
corrplot(cor_loans)

```




Conducting Chi-squared Test on the downsampled dataset (hmda_down)

P-Value for contable1 is lower than .05 therefore we reject the null hypothesis. action_taken and derived_race are not independent of each other

```{r}

contable1= table(hmda_down$derived_race, hmda_down$action_taken)
xkabledply(contable1, title="Contingency table for Loan Approval and Race")
chitest1 = chisq.test(contable1)
chitest1

```



<!-- ```{r} -->
<!-- contable2= table(hmda_down$derived_ethnicity, hmda_down$action_taken) -->
<!-- xkabledply(contable2, title="Contingency table for Loan Approval and Ethnicity") -->
<!-- chitest2 = chisq.test(contable2) -->
<!-- chitest2 -->
<!-- ``` -->



<!-- ```{r} -->
<!-- contable3= table(hmda_down$derived_sex, hmda_down$action_taken) -->
<!-- xkabledply(contable3, title="Contingency table for Loan Approval and Gender") -->
<!-- chitest3 = chisq.test(contable3) -->
<!-- chitest3 -->
<!-- ``` -->

<!-- ```{r} -->
<!-- contable4= table(hmda_down$applicant_age, hmda_down$action_taken) -->
<!-- xkabledply(contable4, title="Contingency table for Loan Approval and Age") -->
<!-- chitest4 = chisq.test(contable4) -->
<!-- chitest4 -->

<!-- ``` -->

### Changing action_taken to factor for logit/recode 1 as denial and 0 as approval because we want to calculate the probability of denial
```{r}
hmda_down$action_taken<-recode(hmda_down$action_taken, "3"="1", "1"="0")
hmda_down$action_taken<-factor(hmda_down$action_taken)
unique(hmda_down$action_taken)
str(hmda_down)

```

### Logit models for action_taken (version with recoded approval/denial)



```{r, results=TRUE}
approval_logit <- glm(action_taken ~ derived_sex+income+applicant_age+interest_rate+
                    loan_amount+property_value+income+derived_sex+loan_amount+income+derived_ethnicity, data = hmda_down, family = "binomial")
summary(approval_logit)
#xkablevif(approval_logit)
hmda_down$action_taken


```

VIF is quite high; not a great model

```{r McFadden_direct}
approvalNullLogit <- glm(action_taken ~ 1, data = hmda_down, family = "binomial")
mcFadden = 1 - logLik(approval_logit)/logLik(approvalNullLogit)
mcFadden
```

Only 27.1 percent of variability is explained by all of the variables we've selected. 

#### Hosmer and Lemeshow test  

The Hosmer and Lemeshow Goodness of Fit test can be used to evaluate logistic regression fit. 

```{r HosmerLemeshow}
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
approveLogitHoslem = hoslem.test(hmda_down$action_taken, fitted(approval_logit)) # Hosmer and Lemeshow test, a chi-squared tests
unloadPkg("ResourceSelection") 
approveLogitHoslem
```

The result is shown here:  
```{r HosmerLemeshowRes, results='markup', collapse=F}
approveLogitHoslem

```

```{r roc_auc}
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(approval_logit, type = "response" )
hmda_down$prob=prob
h <- roc(action_taken~prob, data=hmda_down)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg("pROC")

```



```{r Splitting Model for Train and Test}
set.seed(123)
index <- sample(nrow(hmda_down),nrow(hmda_down)*0.80)
training = hmda_down[index,]
testing = hmda_down[-index,]


hmda_glm0 <- glm(action_taken ~ derived_race + derived_ethnicity + derived_sex + loan_amount+applicant_age+ income+interest_rate+property_value, data = training, family = "binomial")
summary(hmda_glm0)




```
```{r verifying training and testing data}

summary(training) 
summary(testing) 


```






#Note that feature selection gives us this model below, which has the lowest deviance and lowest AIC score at 519.4.

glm(formula = action_taken ~ derived_sex + loan_amount + income + 
    property_value, family = "binomial", data = training)
    

```{r variable/feature selection using backward selection AIC}
hmda_glm_back <- step(hmda_glm0) #backward selections
summary(hmda_glm_back)
hmda_glm_back$deviance
AIC(hmda_glm_back)


```



```{r Predictive Analysis Using Testing dataset on training model}

hmda_glm<-glm(formula = action_taken ~ derived_sex + loan_amount + income + 
    property_value, family = "binomial", data = training)

summary(hmda_glm)


```

