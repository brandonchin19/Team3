---
title: "Technical Analysis: DATS 6101 Final Project"
author: "Brandon Chin, Paul Kelly, Ksenia Shadrina, Luke Wu"
date: "4/11/22"
# date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
library(tidyverse)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "show", message = F)
options(scientific=T, digits = 3) 
#options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

```{r modify_outlierKD2}
#' Original outlierKD function by By Klodian Dhana,
#' https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
#' Modified to have third argument for removing outliers instead of interactive prompt,
#' and after removing outlier, original df will not be changed. The function returns the a df,
#' which can be saved as original df name if desired.
#' Also added QQ-plot in the output, with options to show/hide boxplot, histogram, qqplot.
#' Check outliers, and option to remove them, save as a new dataframe.
#' @param df The dataframe.
#' @param var The variable in the dataframe to be checked for outliers
#' @param rm Boolean. Whether to remove outliers or not.
#' @param boxplt Boolean. Whether to show the boxplot, before and after outliers removed.
#' @param histogram Boolean. Whether to show the histogram, before and after outliers removed.
#' @param qqplt Boolean. Whether to show the qqplot, before and after outliers removed.
#' @return The dataframe with outliers replaced by NA if rm==TRUE, or df if nothing changed
#' @examples
#'   outlierKD2(mydf, height, FALSE, TRUE, TRUE, TRUE)
#'   mydf = outlierKD2(mydf, height, TRUE, TRUE, TRUE, TRUE)
#'   mydfnew = outlierKD2(mydf, height, TRUE)
#' @export
outlierKD2 <- function(df, var, rm=TRUE, boxplt=TRUE, histogram=TRUE, qqplt=TRUE) {
  dt = df # duplicate the dataframe for potential alteration
  var_name <- eval(substitute(var),eval(dt))
  na1 <- sum(is.na(var_name))
  m1 <- mean(var_name, na.rm = T)
  colTotal <- boxplt+histogram+qqplt
  par(mfrow=c(2, max(2,colTotal)), oma=c(0,0,3,0)) # fixed issue with only 0 or 1 chart selected
  if (qqplt) {
    qqnorm(var_name, main = "With outliers")
    qqline(var_name)
  }
  if (histogram) { hist(var_name, main="With outliers", xlab=NA, ylab=NA) }
  if (boxplt) { boxplot(var_name, main="With outliers") }

  outlier <- boxplot.stats(var_name)$out
  mo <- mean(outlier)
  var_name <- ifelse(var_name %in% outlier, NA, var_name)
  if (qqplt) {
    qqnorm(var_name, main = "Without outliers")
    qqline(var_name)
  }
  if (histogram) { hist(var_name, main="Without outliers", xlab=NA, ylab=NA) }
  if (boxplt) { boxplot(var_name, main="Without outliers") }
  
  if(colTotal > 0) {  # if no charts are wanted, skip this section
    title("Outlier Check", outer=TRUE)
    na2 <- sum(is.na(var_name))
    cat("Outliers identified:", na2 - na1, "\n")
    cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "\n")
    cat("Mean of the outliers:", round(mo, 2), "\n")
    m2 <- mean(var_name, na.rm = T)
    cat("Mean without removing outliers:", round(m1, 2), "\n")
    cat("Mean if we remove outliers:", round(m2, 2), "\n")
  }

  # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
  # if(response == "y" | response == "yes"){
  if(rm){
      dt[as.character(substitute(var))] <- invisible(var_name)
      #assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
      cat("Outliers successfully removed", "\n")
      return(invisible(dt))
  } else {
      cat("Nothing changed", "\n")
      return(invisible(df))
  }
}

```

### Research Topic:

In the course of the midterm project research, we discovered that answering our original SMART question would require,at the very least, building a logistic regression model. In addition, we discovered that we had an unbalanced dataset which behooved us to increase our sample size significantly: from 5,000 to 50,000 observations on California.

Based on our initial findings, we proceeded to testing the relationship between rate of loan denials and income of applicant, loan amount, interest rate, property value, gender of applicant, race of applicant, age of applicant, ethnicity of applicant. 

Thus, our SMART question remained largely unchanged:

“Which factors drove denials for mortgages in California in 2019?”

We are using the same dataset as in the midterm project but with 50,000 observations: Federal Financial Institutions Examination Council's (FFIEC) Home Mortgage Disclosure Act (HMDA) dataset from 2019 https://ffiec.cfpb.gov/data-publication/dynamic-national-loan-level- dataset/2019.

Our Github repository address is:https://github.com/brandonchin19/Team3/.

All of the data cleaning procedures remained the same as in the midterm; therefore, our starting point here is 24,018 observations. Specifically, we filtered on the action_taken of denial and approval, removed all the business properties, removed outliers, and removed the missing values. The resulting dataset  "loans.csv" is our starting point. 

### Load in the clean dataset

```{r, results='show'}
loans <- data.frame(read.csv("loans.csv"))
str(loans) #24018 obs. of  9 variables # dropped applicant sex because it's essentially the same as derived sex
```

### Changing vector types 

```{r, results='hide'}
loans$derived_ethnicity = factor(loans$derived_ethnicity)
loans$derived_race = factor(loans$derived_race)
loans$derived_sex = factor(loans$derived_sex)
loans$loan_amount = as.numeric(loans$loan_amount)
loans$interest_rate = as.numeric(loans$interest_rate)
loans$property_value = as.numeric(loans$property_value)
loans$income = as.numeric(loans$income)
loans$applicant_age = factor(loans$applicant_age)
str(loans)
```

### Examine if there are  missing values 

```{r, results='hide'}
missvalue <- is.na(loans)
summary(missvalue) 
```

There are no missing values. 

### Run initial summary statistics: note different scales for income and interest rate

```{r, results='show'}
options(scipen=9, digits = 3) 
Numerical_var <- subset(loans,select=c(loan_amount, income, property_value, interest_rate))
library(kableExtra)
summary_t<-kbl(summary(Numerical_var))%>%
  kable_styling()
summary_t
```

The four numerical variables we are left with are not normally distributed; however, means and medians are close enough. Here, we note the different scales of numerical variables, going forward

### Exploratory Data Analysis

We then do a visual inspection of the new dataset we have, starting with categorical values.

```{r, include=TRUE}
library(ggplot2)
ggplot(loans, aes(x = factor(derived_sex), fill=derived_sex)) +
    geom_bar()+
  labs(title="Graph 1. Applicant sex distribution", x="Applicant Sex", y="Count")

ggplot(loans, aes(x = factor(applicant_age),fill=applicant_age)) +
    geom_bar()+
  labs(title="Graph 2. Applicant age distribution", x="Applicant Age", y="Count")

ggplot(loans, aes(x = factor(derived_ethnicity), fill=derived_ethnicity)) +geom_bar()+
  labs(title="Graph 3. Applicant ethnicity distribution", x="Applicant Ethnicity", y="Count")

ggplot(loans, aes(x = factor(action_taken), fill=action_taken)) +
    geom_bar()+
  labs(title="Graph 4. Action taken distribution", x="Action Taken", y="Count")

ggplot(loans, aes(x = factor(derived_race), fill=derived_race)) +
    geom_bar()+
  labs(title="Graph 5. Derived Race", x="Race of the Applicant", y= "Count")+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```

There are three noteworthy features in the categorical graphs which remain the same as in our midterm project.

First, we note that ethnicity and race variables are overly represented by "Not Hispanic or Latino" and "white" are overrepresented. Secondly, there is an obvious overlap between these two variables with the skew in these two respective categories, and in future research, these two variables should either be combined or used in separate model. Third, the action_taken graph is heavily skewed on approvals.

Next, we proceed to examining the numerical variable histograms.

```{r, include=TRUE}
ggplot(loans,aes(x=loan_amount))+
  geom_histogram(color="black", fill="steelblue")+
  labs(title=" Graph 6. Histogram of Loan Amount", x="Loan Amount in dollars", y="Frequency")

ggplot(loans,aes(x=interest_rate))+
  geom_histogram(color="black", fill="pink")+
  labs(title="Graph 8. Histogram of Interest Rate", x="Interest Rates", y="Frequency")

ggplot(loans,aes(x=income))+
  geom_histogram(color="black", fill="green")+
  labs(title="Graph 9. Histogram of Income", x="Income in dollars", y="Frequency")

ggplot(loans,aes(x=property_value))+
  geom_histogram(color="black", fill="lightblue")+
  labs(title="Graph 10. Histogram of Property Values", x="Property Values in dollars", y="Frequency")

```

Only income and property values appear to approach normal distribution. We proceed without taking any additional outliers out because balancing of the data will require to remove additonal data.

We decided to run to chi-square tests on the unbalanced data first, and contrary to the results we got in the midterm project, all tests ran well and showed statistical significance this time.

```{r, results='show'}

contable1= table(loans$derived_race, loans$action_taken)
xkabledply(contable1, title="Contingency table for Loan Approval and Race")
chitest1 = chisq.test(contable1)
chitest1

```

```{r, results='show'}
contable2= table(loans$derived_ethnicity, loans$action_taken)
xkabledply(contable2, title="Contingency table for Loan Approval and Ethnicity")
chitest2 = chisq.test(contable2)
chitest2
```

```{r, results='show'}
contable3= table(loans$derived_sex, loans$action_taken)
xkabledply(contable3, title="Contingency table for Loan Approval and Gender")
chitest3 = chisq.test(contable3)
chitest3
```

```{r, results='show'}
contable4= table(loans$applicant_age, loans$action_taken)
xkabledply(contable4, title="Contingency table for Loan Approval and Age")
chitest4 = chisq.test(contable4)
chitest4

```

Despite the chi-square tests running well and confirming that denial or approvals are not independent of personal attributes of the applicants: race, ethnicity, age, and sex.

We proceed to balancing the dataset

### Balancing the dataset

```{r}
hmda <- data.frame(loans)


hmda$approval<- ifelse(hmda$action_taken=="1", "Approved","Denied")
hmda$approval<- factor(hmda$approval)
str(hmda)
table(hmda$approval)

```

The number of approvals is over three times higher than denials as shown in the table below.

```{r, results='show'}
with(hmda,
     {
       print(table(derived_race))
       print(table(approval))
     }
)

```

To balance the dataset, the downsampling technique is used. Balancing the dataset leaves us with 10,952 observations.

```{r, results='show'}
Approved<- which(hmda$action_taken=="1")
Denied<- which(hmda$action_taken=="3")

length(Approved)
length(Denied)

#The technique
approved_downsample<-sample(Approved,length(Denied))
hmda_down<- hmda[c(approved_downsample, Denied),]
str(hmda_down)
#10952 obs. of  11 variables

```

### The Chi Square Tests on balanced dataset

We then, rerun the Chi-Square tests on the balanced dataset (hmda_down)

P-Values for all of the tests are once again lower than .05 therefore we reject the null hypotheses in all four cases again.

Ran 4 Chi Squared Test
Action_Taken (our Approval-Denial Variable) and Race
At the .05 significance level proved that  Action_taken was not independent of Race
 p-value <2e-16
Action_Taken and Ethnicity 
At the .05 significance level proved that  Action_taken was not independent of Ethnicity 
 p-value = 0.00001
Action_Taken and Gender
At the .05 significance level proved that  Action_taken was not independent of Gender
p-value <2e-16
Action_Taken and Age
At the .05 significance level proved that  Action_taken was not independent of Age
 p-value = 0.00000000001

```{r, results='show'}

contable1= table(hmda_down$derived_race, hmda_down$action_taken)
xkabledply(contable1, title="Balanced Data: Contingency table for Loan Approval and Race")
chitest1 = chisq.test(contable1)
chitest1

```

```{r, results='show'}
contable2= table(hmda_down$derived_ethnicity, hmda_down$action_taken)
xkabledply(contable2, title="Balanced Data: Contingency table for Loan Approval and Ethnicity")
chitest2 = chisq.test(contable2)
chitest2
```

```{r, results='show'}
contable3= table(hmda_down$derived_sex, hmda_down$action_taken)
xkabledply(contable3, title="Balanced Data: Contingency table for Loan Approval and Gender")
chitest3 = chisq.test(contable3)
chitest3
```

```{r, results='show'}
contable4= table(hmda_down$applicant_age, hmda_down$action_taken)
xkabledply(contable4, title="Balanced Data: Contingency table for Loan Approval and Age")
chitest4 = chisq.test(contable4)
chitest4

```

### Building the logit model

As a first step to building the logit model, we recode it to 1 as denial and 0 as approval because we want to calculate the probability of denial to answer the smart question

```{r, results='hide'}
hmda_down$action_taken<-dplyr::recode(hmda_down$action_taken, "3"="1", "1"="0")
unique(hmda_down$action_taken)
hmda_down$action_taken<-factor(hmda_down$action_taken)
str(hmda_down)

myvars <- names(hmda_down) %in% c("approval") # dropping approval since we don't need this variable anymore
newdata <- hmda_down[!myvars]
hmda_down<-newdata
str(hmda_down)
```

### Logit models for action_taken (version with recoded approval/denial)

We run a model with all of the variables we've included in the analysis.

```{r, results='show'}
###using all the variables of interest first
denial_logit <- glm(action_taken ~ derived_race + derived_ethnicity + derived_sex + loan_amount+applicant_age+ income+interest_rate+property_value, data = hmda_down, family = "binomial")
summary(denial_logit)

```

The model shows a lot of significant coefficients. Significance and signs of some of the coefficients are expected, such as an inverse relationship between income and denial rates: the higher the income, the lower the probability (log of odds) of denial for a loan. The higher the interest rate, the higher the chances of loan denials. 

However, upon inspection, multicollinearity is quite high and might be related to the inherent interrelatedness of the personal attribute variables. 

```{r, results='show'}
xkablevif(denial_logit) #check for multicollinearity
```

We proceed to other tests for the model.

```{r, results='show'}
denialNullLogit <- glm(action_taken ~ 1, data = hmda_down, family = "binomial")
mcFadden = 1 - logLik(denial_logit)/logLik(denialNullLogit)
mcFadden
```

The McFadden score is extremely low. Our model explains only about 15.5 percent of "variability" in the outcome variable--the probability of denial.

```{r, results='show'}
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
approveLogitHoslem = hoslem.test(hmda_down$action_taken, fitted(denial_logit)) # Hosmer and Lemeshow test, a chi-squared tests
unloadPkg("ResourceSelection") 
approveLogitHoslem
```

Per Hosmer and Leshow, our model is quite poor; p-value is extremely low.

```{r HosmerLemeshowRes, results='markup', collapse=F}
approveLogitHoslem

```

Finally, we look at the area under the curve, which is 0.775 in our case. We confirm that the model is a poor fit.

```{r, results='show'}
loadPkg("pROC") 
prob=predict(denial_logit, type = "response" )
hmda_down$prob=prob
h <- roc(action_taken~prob, data=hmda_down)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg("pROC")

```

We then try to look for a new model using feature selection for logistic regressions.

### Feature Selection for Logit

```{r}
library(bestglm)
str(hmda_down)
fs_hmda_down<-hmda_down
fs_hmda_down$y = hmda_down[,4] 
str(fs_hmda_down)
fs_hmda_down<-fs_hmda_down[c(1:3, 5:9, 11)] #subset for feature selection model that does not include action_taken

str(fs_hmda_down)

res.bestglm <- bestglm(Xy = fs_hmda_down, family = binomial,
            IC = "AIC",                 # Information criteria for
            method = "exhaustive")

summary(res.bestglm)

res.bestglm$BestModels
summary(res.bestglm$BestModels)
#https://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html
```

Our test indicates that the first model we ran is the best model. The lowest AIC score is 12871. We test the "next best" model, ust in case-the model that excludes property value.

Feature selection shows that the lowest AIC would use all variables, but not certain this is true; trying the second best without the derived sex

```{r, include=TRUE}
denial_logit1 <- glm(action_taken ~ derived_race + derived_ethnicity + loan_amount+applicant_age+ income+interest_rate+property_value, data = hmda_down, family = "binomial")
summary(denial_logit1)

unique(hmda_down$action_taken)
```

We proceed to testing the fit of the second model.

We find that McFadden is not improved, and the area under the curve is now lower.

```{r McFadden_direct 2}
denialNullLogit <- glm(action_taken ~ 1, data = hmda_down, family = "binomial")
mcFadden = 1 - logLik(denial_logit1)/logLik(denialNullLogit)
mcFadden
```

```{r roc_auc 2}
loadPkg("pROC") 
prob=predict(denial_logit1, type = "response" )
hmda_down$prob=prob
h <- roc(action_taken~prob, data=hmda_down)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg("pROC")

```
### Splitting Model for Train and Test

Sampling down: drawing a random sample also making training dataset from a random sample of 80% of the data and testing dataset from 20% of the data. Below we run our training dataset through our previous model and the results are similar, the residual deviance is lower than the null deviance at the same difference as with the full dataset indicating that the model is comparable, it carries all the same levels of variables and variables that were statistically significant in the previous model. 

```{r, results='show'}
set.seed(123)
index <- sample(nrow(hmda_down),nrow(hmda_down)*0.80)
training = hmda_down[index,]
testing = hmda_down[-index,]


hmda_glm0 <- glm(action_taken ~ derived_race + derived_ethnicity + derived_sex + loan_amount+applicant_age+ income+interest_rate+property_value, data = training, family = "binomial")
summary(hmda_glm0)




```

### Verifying training and testing data

Veiwing training and testing dataset to ensure that it represents correct portions of the original dataset.

```{r  results='show'}

summary(training) 
summary(testing) 

```

### Feature Selection

This is a feature selection done in a different way to confirm the previous feature selection to ensure we are using the correct model.  The model results are similar and the full model it gives us the same model to use as the best fit model for our variables.

```{r variable/feature selection using backward selection AIC}
hmda_glm_back <- step(hmda_glm0) #backward selections
summary(hmda_glm_back)
hmda_glm_back$deviance
AIC(hmda_glm_back)
```

### Predictive Analysis & Confusion Matrix

Here we run our testing dataset through our model and look into the results. The results are simlair and the residual deviance is the same. Next we created an expected values dataframe and a predicted probability dataset to make a confusion matrix to see how well our model is doing. 

The Accuracy Score is 72% and recall rate is 73%, this model is doing a decent job of predicting our testing dataset. It is important to note that we are focus on drivers for denials in our research but is shows that our model predicts approvals at a slightly higher rate than denials. 

```{r results='show'}
library(caret)
glm_fit = glm(action_taken ~ derived_race + derived_ethnicity + loan_amount+applicant_age+ income+interest_rate+property_value, data = training, family = "binomial")
glm_fit


glm_probs = data.frame(probs = predict(glm_fit, 
                                       newdata = testing, 
                                       type="response"))

glm_pred = glm_probs %>%
  mutate(pred = ifelse(probs>.5, "1", "0"))



pc<-factor(glm_pred$pred)
e<- (testing$action_taken)


table(pc)
table(e)


confusionMatrix(data=pc, reference = e)


x<-data.frame("target"=c(1,0,1,0), "prediction"=c(1,1,0,0), "n"=c(817,306,288,786))

library(ggplot2)

ggplot(x, aes(x = target, y = prediction, fill = n)) +
  geom_tile(color = "black") +
  geom_text(aes(label = n), color = "Yellow", size = 4) +
  coord_fixed()+
  scale_fill_gradient2(low = "#075AFF",
                       mid = "#FFFFCC",
                       high = "Purple") +
     scale_y_continuous(trans = "reverse", breaks = unique(x$prediction))

 



```

### Decision trees 

Split data(training:testing = 80%:20%)

```{r results='show'}
n = nrow(loans)
split = sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.8, 0.2))

tree.training = loans[split, ]
tree.testing = loans[!split, ]
```

### Building model.

```{r results='show'}
library(rpart)
library(rpart.plot)
fit <- rpart(action_taken~., data = tree.training, method = 'class',control = ("maxdepth = 4"))
rpart.plot(fit, extra = 106)

```

At the top (node1), it is the overall probability of loan approval. It shows the proportion of applicant that get approved for their loan. 50 percent of applicant get approved.

Node1 asks whether the interest rate is less than 4.5, If yes, 37 percent of applicants have no chance to have their loan approved; if no, 63 percent of applicants have 79 percent of chance to have their loan denied. 

In the node2, asking if the interest rate is equal or greater than 4.5, If yes, 50 percent of applicant have 100 percent of chance to have their loan denied; if no, 13 percent of applicants will not have their loan approved. 

### Making a prediction

```{r results='show'}
predict_unseen <-predict(fit, tree.testing, type = 'class')
summary(predict_unseen)
```

### Create a table to count how many applicants are classified as approval and denied

```{r results='show'}
table_mat <- table(tree.testing$action_taken, predict_unseen )
table_mat

```

### Measure performance:confustion martrix

```{r results='show'}
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)#0.999
```

### Conclusions

We have a score of 99 percent for the test set.

Via feature selection, we know ethnicity, race, sex, loan amount, interest rate, property value, income, and age are important.

An earlier decision tree analysis highlighted income and loan amount.
Income and loan amount are important variables when determining mortgage approval and rejection, which feels true. In the real world, though, income is directly impacted by the race, ethnicity and sex of the individual in question. Bottom line: it’s difficult to separate these things when they are so interrelated.

A second decision tree, highlighted in this presentation, surfaced interest rate as important.