---
title: "DATS 6101 Final Project"
author: "Brandon Chin, Paul Kelly, Ksenia Shadrina, Luke Wu"
date: "4/11/22"
# date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
library(tidyverse)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
#options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

```{r modify_outlierKD2}
#' Original outlierKD function by By Klodian Dhana,
#' https://www.r-bloggers.com/identify-describe-plot-and-remove-the-outliers-from-the-dataset/
#' Modified to have third argument for removing outliers instead of interactive prompt,
#' and after removing outlier, original df will not be changed. The function returns the a df,
#' which can be saved as original df name if desired.
#' Also added QQ-plot in the output, with options to show/hide boxplot, histogram, qqplot.
#' Check outliers, and option to remove them, save as a new dataframe.
#' @param df The dataframe.
#' @param var The variable in the dataframe to be checked for outliers
#' @param rm Boolean. Whether to remove outliers or not.
#' @param boxplt Boolean. Whether to show the boxplot, before and after outliers removed.
#' @param histogram Boolean. Whether to show the histogram, before and after outliers removed.
#' @param qqplt Boolean. Whether to show the qqplot, before and after outliers removed.
#' @return The dataframe with outliers replaced by NA if rm==TRUE, or df if nothing changed
#' @examples
#'   outlierKD2(mydf, height, FALSE, TRUE, TRUE, TRUE)
#'   mydf = outlierKD2(mydf, height, TRUE, TRUE, TRUE, TRUE)
#'   mydfnew = outlierKD2(mydf, height, TRUE)
#' @export
outlierKD2 <- function(df, var, rm=TRUE, boxplt=TRUE, histogram=TRUE, qqplt=TRUE) {
  dt = df # duplicate the dataframe for potential alteration
  var_name <- eval(substitute(var),eval(dt))
  na1 <- sum(is.na(var_name))
  m1 <- mean(var_name, na.rm = T)
  colTotal <- boxplt+histogram+qqplt
  par(mfrow=c(2, max(2,colTotal)), oma=c(0,0,3,0)) # fixed issue with only 0 or 1 chart selected
  if (qqplt) {
    qqnorm(var_name, main = "With outliers")
    qqline(var_name)
  }
  if (histogram) { hist(var_name, main="With outliers", xlab=NA, ylab=NA) }
  if (boxplt) { boxplot(var_name, main="With outliers") }

  outlier <- boxplot.stats(var_name)$out
  mo <- mean(outlier)
  var_name <- ifelse(var_name %in% outlier, NA, var_name)
  if (qqplt) {
    qqnorm(var_name, main = "Without outliers")
    qqline(var_name)
  }
  if (histogram) { hist(var_name, main="Without outliers", xlab=NA, ylab=NA) }
  if (boxplt) { boxplot(var_name, main="Without outliers") }
  
  if(colTotal > 0) {  # if no charts are wanted, skip this section
    title("Outlier Check", outer=TRUE)
    na2 <- sum(is.na(var_name))
    cat("Outliers identified:", na2 - na1, "\n")
    cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(var_name))*100, 1), "\n")
    cat("Mean of the outliers:", round(mo, 2), "\n")
    m2 <- mean(var_name, na.rm = T)
    cat("Mean without removing outliers:", round(m1, 2), "\n")
    cat("Mean if we remove outliers:", round(m2, 2), "\n")
  }

  # response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
  # if(response == "y" | response == "yes"){
  if(rm){
      dt[as.character(substitute(var))] <- invisible(var_name)
      #assign(as.character(as.list(match.call())$dt), dt, envir = .GlobalEnv)
      cat("Outliers successfully removed", "\n")
      return(invisible(dt))
  } else {
      cat("Nothing changed", "\n")
      return(invisible(df))
  }
}

```

### Research Topic:

There is considerable evidence indicating lending disparities throughout the United States. (Steil et. al: 2018). For our research topic, we will explore lending practices in one of the fastest appreciating real estate markets over the past thirty years - the state of California. Specifically, we aim to look at the factors that are associated with denials for non-commercial mortgage loans.

Our SMART question is:

“Which factors drove denials for mortgages in California in 2019?”

To answer this question, we are using the Federal Financial Institutions Examination Council's (FFIEC) Home Mortgage Disclosure Act (HMDA) dataset from 2019, located here: https://ffiec.cfpb.gov/data-publication/dynamic-national-loan-level-dataset/2019. We are  focusing on a subset of data of 10,000 observations from 2019 that we will further filter on California leaving us with 5,196 observations.

Our Github repository address is:https://github.com/brandonchin19/Team3/.

### Load in dataset

```{r, results='hide'}
hmda_ca <- data.frame(read.csv("hmda_ca_new.csv"))
str(hmda_ca)
```

### Check number of rows

```{r}
dim(hmda_ca)
```


### Check head and tail of dataframe

```{r}
xkabledplyhead(hmda_ca,5)
xkabledplytail(hmda_ca,5)
```


### Subsetting to California Only; non-business properties; principal residences only

```{r}
hmda_ca <- subset(hmda_ca,business_or_commercial_purpose=="2")
str(hmda_ca)
```


```{r}
hmda_ca <- subset(hmda_ca,business_or_commercial_purpose=="2")
str(hmda_ca) #48030 obs. of  99 variables
```




### Subsetting principal residences only; tail and head check to make sure that the geography is widespread/our sample is "random"; there are now 59 distinct counties.

```{r}

hmda_ca <- subset(hmda_ca,occupancy_type=="1")
dim(hmda_ca) #45735    99
loadPkg("sqldf")
sqldf("select count(distinct(county_code)) from hmda_ca")
unloadPkg("sqldf")
```

### Subsetting to only relevant actions: denial or approval

```{r}
hmda_ca1<-hmda_ca%>%filter(action_taken %in% c("1", "3"))
hmda_ca<-hmda_ca1
dim(hmda_ca) #30661    99
```


### Subsetting relevant variables for answering the SMART question
```{r}
hmda_ca_final <- hmda_ca[c(10,11,12,13,22,24,39,46,50,62,74,78)]
str(hmda_ca_final) #30661 obs. of  12 variables:
```
##Clean the age group variable (some "8888" value)
```{r}
unique(hmda_ca_final$applicant_age)
hmda_age<-hmda_ca_final%>%filter(applicant_age !="8888")
unique(hmda_age$applicant_age)
hmda_ca_final<-hmda_age
```
### Changing vector types

```{r}
hmda_ca_final_1 = hmda_ca_final
hmda_ca_final_1$derived_ethnicity = factor(hmda_ca_final$derived_ethnicity)
hmda_ca_final_1$derived_race = factor(hmda_ca_final$derived_race)
hmda_ca_final_1$derived_sex = factor(hmda_ca_final$derived_sex)
#hmda_ca_final_1$action_taken = factor(hmda_ca_final$action_taken)
hmda_ca_final_1$loan_amount = as.numeric(hmda_ca_final$loan_amount)
hmda_ca_final_1$interest_rate = as.numeric(hmda_ca_final$interest_rate)
hmda_ca_final_1$property_value = as.numeric(hmda_ca_final$property_value)
hmda_ca_final_1$income = as.numeric(hmda_ca_final$income)
hmda_ca_final_1$applicant_age = factor(hmda_ca_final$applicant_age)
str(hmda_ca_final_1)
```

#Examine and filter out the missing values 

```{r, include=TRUE}
missvalue <- is.na(hmda_ca_final_1)
summary(missvalue)
#There are 30,661 observations after we've filtered on all of the relevant fields. 
#Missing values are present in interest_rate: 6,919; property_value: 1,734, and income: 1,302.
```


```{r, include=TRUE}
hmda_ca_final_2 <- hmda_ca_final_1$interest_rate[is.na(hmda_ca_final_1$interest_rate)] <- mean(hmda_ca_final_1$interest_rate, na.rm = TRUE)
hmda_ca_final_2 <- hmda_ca_final_1 %>% drop_na(applicant_ethnicity.1)
hmda_ca_final_2 <- na.omit(hmda_ca_final_1, cols="income")
missvalue1 <- is.na(hmda_ca_final_2)
summary(missvalue1)
#After cleaning out the missing values, we are left with 28,695 observations
```
##Run initial summary statistics

```{r, results='show'}
options(scipen=9, digits = 3) 
Numerical_var <- subset(hmda_ca_final_2,select=c(loan_amount, income, property_value, interest_rate))
library(kableExtra)
summary_t<-kbl(summary(Numerical_var))%>%
  kable_styling()
summary_t
```



### VI. Exploratory Data Analysis

#Exploring the categorical values first

```{r, include=TRUE}
library(ggplot2)
ggplot(hmda_ca_final_2, aes(x = factor(derived_sex))) +
    geom_bar(color="black", fill="antiquewhite2")+
  labs(title="Graph 1. Applicant sex distribution", x="Applicant Sex", y="Count")

ggplot(hmda_ca_final_2, aes(x = factor(applicant_age))) +
    geom_bar(color="black", fill="bisque3")+
  labs(title="Graph 2. Applicant age distribution", x="Applicant Age", y="Count")

ggplot(hmda_ca_final_2, aes(x = factor(derived_ethnicity))) +
    geom_bar(color="black", fill="cornsilk3")+
  labs(title="Graph 3. Applicant ethnicity distribution", x="Applicant Ethnicity", y="Count")

ggplot(hmda_ca_final_2, aes(x = factor(action_taken))) +
    geom_bar(color="black", fill="azure3")+
  labs(title="Graph 4. Action taken distribution", x="Action Taken", y="Count")

ggplot(hmda_ca_final_2, aes(x = factor(derived_race))) +
    geom_bar(color="black", fill="azure3")+
  labs(title="Graph 5. Derived Race", x="Race of the Applicant", y= "Count")+theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

```


Next, we proceed to exploring the numerical data for normality and outliers. Graphs 8-10 indicate that none of the numerical variables are normally distributed.

```{r, include=TRUE}
ggplot(hmda_ca_final_2,aes(x=loan_amount))+
  geom_histogram(color="black", fill="steelblue")+
  labs(title=" Graph 6. Histogram of Loan Amount", x="Loan Amount in dollars", y="Frequency")

ggplot(hmda_ca_final_2,aes(x=interest_rate))+
  geom_histogram(color="black", fill="pink")+
  labs(title="Graph 8. Histogram of Interest Rate", x="Interest Rates", y="Frequency")

ggplot(hmda_ca_final_2,aes(x=income))+
  geom_histogram(color="black", fill="azure3")+
  labs(title="Graph 9. Histogram of Income", x="Income in dollars", y="Frequency")

ggplot(hmda_ca_final_2,aes(x=property_value))+
  geom_histogram(color="black", fill="lightblue")+
  labs(title="Graph 10. Histogram of Property Values", x="Property Values in dollars", y="Frequency")
```


As another check, we inspect qqnorm plots and confirm that the distributions are not normal(Graph 11-14)

```{r, include=TRUE}
qqnorm(hmda_ca_final_2$interest_rate,
       main="Graph 11. QQ Plot of Interest Rates",
       ylab="Interest Rate",
       col="pink")
qqline(hmda_ca_final_2$interest_rate)

qqnorm(hmda_ca_final_2$income,
       main="Graph 13. QQ Plot of Income",
       ylab="Income",
       col="green")
qqline(hmda_ca_final_2$income)

qqnorm(hmda_ca_final_2$property_value,
       main="Graph 12. QQ Plot of Property Values",
       ylab="Property Value",
       col="blue")
qqline(hmda_ca_final_2$property_value)

qqnorm(hmda_ca_final_2$loan_amount,
       main="Graph 14.QQ Plot of Loan Amount",
       ylab="Loan Amount",
       col="purple")
qqline(hmda_ca_final_1$loan_amount)
```


Next, we remove outliers and check for normality again.
```{r, include=TRUE}
hmda_ca_final_no_outliers_3 <- outlierKD2(hmda_ca_final_2,interest_rate)
```

```{r, include=TRUE}
hmda_ca_final_no_outliers_4 <- outlierKD2(hmda_ca_final_no_outliers_3,property_value)
```


```{r, include=TRUE}
hmda_ca_final_no_outliers_5 <- outlierKD2(hmda_ca_final_no_outliers_4,income)
```

```{r, include=TRUE}
loans <- outlierKD2(hmda_ca_final_no_outliers_5,loan_amount)
```

We then reexamine the variables for normality after removing the outliers.

```{r, include=TRUE}
qqnorm(loans$interest_rate,
       main="Graph 15. QQ Plot of Interest Rates",
       ylab="Interest Rate",
       col="pink")
qqline(loans$interest_rate)

qqnorm(loans$property_value,
       main="Graph 16. QQ Plot of Property Values",
       ylab="Property Value",
       col="blue")
qqline(loans$property_value)

qqnorm(loans$income,
       main="Graph 17.QQ Plot of Income",
       ylab="Income",
       col="green")
qqline(loans$income)

qqnorm(loans$loan_amount,
       main="Graph 18. QQ Plot of Loan Amount",
       ylab="Loan Amount",
       col="purple")
qqline(loans$loan_amount)
```

The values do not appear to be normally distributed even after removing the outliers

```{r, include=TRUE}
Numerical_var <- subset(loans,select=c(loan_amount, income, property_value, interest_rate))
#str(Numerical_var)
library(kableExtra)
summary_t<-kbl(summary(Numerical_var))%>%
  kable_styling()
summary_t

# GROUP: what's with the NAs in the summary table after we have cleaned the NAs out?
```
However, the means and the medians are fairly close in all instances; do we keep removing the outliers?

```{r, include=TRUE} 
#this chunk is for visual inspection only: EITHER DELETE or FORMAT before the final submission
boxplot(loans$interest_rate)
boxplot(loans$loan_amount)
boxplot(loans$property_value)
boxplot(loans$income)
```


## Starting the tests
```{r, include=TRUE}
#library(lattice)
#dim(loans) #28695    12
#pairs(loans) #let's not run this for the final project; doesn't tell us much
#str(loans)

library(epiDisplay)
tab1(loans$derived_race, sort.group = "decreasing", cum.percent = TRUE)
```


##Preparing data for corrplot
```{r, include=FALSE}
summary(loans$derived_race)
loans$Black<- ifelse(loans$derived_race=="Black or African American", 1, 0)
loans$AIAN<- ifelse(loans$derived_race=="American Indian or Alaska Native", 1, 0)
loans$NHPI<- ifelse(loans$derived_race=="Native Hawaiian or Other Pacific Islander", 1, 0)
loans$Asian<- ifelse(loans$derived_race=="Asian", 1, 0)
loans$Joint<- ifelse(loans$derived_race=="Joint", 1, 0)
loans$N_A<- ifelse(loans$derived_race=="Race Not Available", 1, 0)
loans$two_or<- ifelse(loans$derived_race=="2 or more minority races", 1, 0)
str(loans)
```

```{r, include=FALSE}
loans_all_num<-loans[c(4:17)]
loans_all_num$applicant_age =as.numeric(loans$applicant_age)
#str(loans_all_num)
```


```{r, include=TRUE}
cor_loans<-cor(loans_all_num, use="complete.obs")
xkabledply(cor_loans)
loadPkg("corrplot")
corrplot(cor_loans)

```

### Chi-Square Test for Loan Approval and Race: result=>reject the null of independence: all tests REJECT THE NULL HYPOTHESIS OF INDEPENDENCE NOW THAT WE HAVE MORE DATA 
``` {r, results=TRUE}
contable1 = table(loans$derived_race, loans$action_taken)
xkabledply(contable1, title="Contingency table for Loan Approval and Race")
chitest1 = chisq.test(contable1)
chitest1
```

``` {r, results=TRUE}
contable2 = table(loans$derived_sex, loans$action_taken)
xkabledply(contable2, title="Contingency table for Loan Approval and Sex")
chitest2 = chisq.test(contable2)
chitest2
```

``` {r, results=TRUE}
contable3 = table(loans$derived_ethnicity, loans$action_taken)
xkabledply(contable3, title="Contingency table for Loan Approval and Ethnicity")
chitest3 = chisq.test(contable3)
chitest3
```

``` {r, results=TRUE}
contable4 = table(loans$applicant_age, loans$action_taken)
xkabledply(contable4, title="Contingency table for Loan Approval and Age")
chitest4 = chisq.test(contable4)
chitest4
```



#Adding New Code for final and downsampling

```{r}
hmda <- data.frame(loans)
str(hmda$action_taken)


hmda$approval<- ifelse(hmda$action_taken=="1", "Approved","Denied")
hmda$approval<- factor(hmda$approval)
str(hmda)
table(hmda$approval)

#Approved   Denied 
   #22773     5922 

```


Checking frequency of approval variable and derived_race variable

```{r}
with(hmda,
     {
       print(table(derived_race))
       print(table(approval))
     }
)




```
Creating DownSample of hmda dataset, 
the Approved far outnumber the Denied making this dataset severely unbalanced 
  #Approved: 22773     Denied: 5922 
```{r}
Approved<- which(hmda$action_taken=="1")
Denied<- which(hmda$action_taken=="3")

length(Approved)
length(Denied)

#The technique
approved_downsample<-sample(Approved,length(Denied))
hmda_down<- hmda[c(approved_downsample, Denied),]
str(hmda_down)

```


```{r}

hmda_downnum<-hmda_down[c(4:11, 13:17, 19)]
hmda_downnum$approved<- ifelse(hmda_downnum$action_taken=="1", 1, 0)
hmda_downnum$denied<- ifelse(hmda_downnum$action_taken=="3", 1, 0)

str(hmda_downnum)
```


```{r}
cor_loans<-cor(hmda_downnum, use="complete.obs")
xkabledply(cor_loans)
loadPkg("corrplot")
corrplot(cor_loans)

```




Conducting Chi-squared Test on the downsampled dataset (hmda_down)

P-Value for contable1 is lower than .05 therefore we reject the null hypothesis. action_taken and derived_race are not independent of each other

```{r}

contable1= table(hmda_down$derived_race, hmda_down$action_taken)
xkabledply(contable1, title="Contingency table for Loan Approval and Race")
chitest1 = chisq.test(contable1)
chitest1

```



```{r}
contable2= table(hmda_down$derived_ethnicity, hmda_down$action_taken)
xkabledply(contable2, title="Contingency table for Loan Approval and Ethnicity")
chitest2 = chisq.test(contable2)
chitest2
```



```{r}
contable3= table(hmda_down$derived_sex, hmda_down$action_taken)
xkabledply(contable3, title="Contingency table for Loan Approval and Gender")
chitest3 = chisq.test(contable3)
chitest3
```

```{r}
contable4= table(hmda_down$applicant_age, hmda_down$action_taken)
xkabledply(contable4, title="Contingency table for Loan Approval and Age")
chitest4 = chisq.test(contable4)
chitest4

```

### Full linear model for action_taken


```{r}
# full_model <- lm(action_taken~., data = loans)
# summary(full_model)
```

### Removing specific race fields due to NAs, running model again

```{r}
loans_for_modelling <- loans[1:12]
#full_model_2 <- lm(action_taken~., data = loans_for_modelling)
#summary(full_model_2)
```

#```{r}
#pcr.fit=pcr(action_taken~.,data=loans_for_modelling,scale=FALSE,validation ="CV")
#ezids::xkabledplyhead(loans_for_modelling)
#summary(pcr.fit)
#```


```{r}
#str(loans_for_modelling)
```


### Linear models for action_taken

```{r}
action_taken_fit1 <- lm(action_taken ~ derived_race, data = loans_for_modelling, family = "binomial")
summary(action_taken_fit1)
xkablevif(action_taken_fit1)
```

```{r}
action_taken_fit2 <- lm(action_taken ~ derived_race + derived_ethnicity, data = loans_for_modelling, family = "binomial")
summary(action_taken_fit2)
```

```{r}
action_taken_fit3 <- lm(action_taken ~ derived_race + derived_ethnicity + derived_sex, data = loans_for_modelling, family = "binomial")
summary(action_taken_fit3)
xkablevif(action_taken_fit3)
```

```{r}
action_taken_fit4 <- lm(action_taken ~ derived_race + derived_ethnicity + derived_sex + applicant_age, data = loans_for_modelling, family = "binomial")
summary(action_taken_fit4)
xkablevif(action_taken_fit4)
```

The multiple R^2s increase with each successive model, indicating slight improvements from action_taken_fit1 through action_taken_fit4.

### ANOVA comparison of models

```{r}
anova(action_taken_fit1,action_taken_fit2,action_taken_fit3,action_taken_fit4) -> anovaRes
str(anovaRes)
xkabledply(anovaRes, title = "ANOVA comparison between the models")
```

The residuals decrease with each successive linear model. This suggests that action_taken_fit4 is a better regression model for explaining this data.

######KS############try with recoding the ##############
### Changing action_taken to factor for logit/recode 1 as denial and 0 as approval because we want to calculate the probability of denial



```{r}
hmda_down$action_taken<-recode(hmda_down$action_taken, "3"="1", "1"="0")
hmda_down$action_taken<-factor(hmda_down$action_taken)
unique(hmda_down$action_taken)
```

### Logit models for action_taken (version with recoded approval/denial)

```{r}
###using all the variables of interest first
approval_logit <- glm(action_taken ~ derived_race + derived_ethnicity + derived_sex + loan_amount+applicant_age+ income+interest_rate+property_value, data = hmda_down, family = "binomial")
summary(approval_logit)
xkablevif(approval_logit)
```
VIF is quite high; likely an overfit

```{r McFadden_direct}
approvalNullLogit <- glm(action_taken ~ 1, data = hmda_down, family = "binomial")
mcFadden = 1 - logLik(approval_logit)/logLik(approvalNullLogit)
mcFadden
```

Only 27.1 percent of variability is explained by all of the variables we've selected. 



```{r}
approval_logit_1 <- glm(action_taken ~ derived_race, data = hmda_down, family = "binomial")
summary(approval_logit_1)
xkablevif(approval_logit_1)
```


```{r}
loans_for_modelling = loans_for_modelling
loans_for_modelling$action_taken = factor(loans_for_modelling$action_taken)
```

```{r}
hmda_down = hmda_down
hmda_down$action_taken = factor(hmda_down$action_taken)
```

### Logit models for action_taken


```{r}
approval_logit <- glm(action_taken ~ derived_race + derived_ethnicity + derived_sex + loan_amount + income, data = hmda_down, family = "binomial")
summary(approval_logit)
xkablevif(approval_logit)
```

```{r}
approval_logit_1 <- glm(action_taken ~ derived_race, data = hmda_down, family = "binomial")
summary(approval_logit_1)
xkablevif(approval_logit_1)
```






















### Sample predictions


1. What are the chances of loan approval for a Black, non-Latino woman with an income of 45,000 per year for a requested loan amount of 100,000?
2. What are the chances of loan approval for a Black borrower?
3. What are the chances of loan approval for a white borrower?


```{r}
#define new observations
newdata1 = data.frame(derived_race = 'Black or African American', derived_sex = 'Female', loan_amount = 100000, derived_ethnicity = 'Not Hispanic or Latino', income = 45000)
newdata2 = data.frame(derived_race = 'Black or African American')
newdata3 = data.frame(derived_race = 'White')


newdata1 = predict(approval_logit, newdata1, type="response")
newdata2 = predict(approval_logit_1, newdata2, type="response")
newdata3 = predict(approval_logit_1, newdata3, type="response")
newdata1
newdata2
newdata3
```

1. Close to 0% (model too complicated/problem with those numerical values just not being present in the data?)
2. 68.1%
3. 44%


```{r}
summary(hmda_down$action_taken)
```

#SideNotes on ANOVA Testing 
These are some constructs of my ANOVA test I left them here just incase I need to revist them, these might not be perfect because I ran multiple different variants, I assume it will not be used ultimately and if we need too we can run chi-squared on the balanced dataset.

Conducting ANOVA Test on action_taken variable and income, saved test as anova

anova= aov(income ~ approval, data=hmda)
str(anova)



#Plotting ANOVA TEST



loadPkg("ggplot2")
ggplot(hmda, aes(x=approval, y=income)) +
  geom_boxplot( colour=c("#ff0000" ,"#11cc11")) +
  labs(title="Income difference between Approved and Denied Apps", x="Approval Status", y = "Income")


 #plot(income ~ action_taken, data=hmda)
anova= aov(action_taken ~ Black, data=hmda)
# anovaRes    # this does not give the easy-to-read result of the aov analysis
names(anova)
# summary(anovaRes)
xkabledply(anova) # same exact result with or without re-ordering.


